{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bert for Question Answering\n",
    "### Credit to Google's CoLab for helping providing code to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/kaleb/.local/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-large-uncased-whole-word-masking-finetuned-squad' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "# bert_model = './bert'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(bert_model)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 130 tokens.\n"
     ]
    }
   ],
   "source": [
    "# question = \"When did the British held American colonies declare their independence?\"\n",
    "question = \"What did the American people declare their independence from?\"\n",
    "answer_text = \"Independence Day, known colloquially as the Fourth of July, is a Federal Holiday in the United States which commemorates the ratification of the Declaration of Independence by the Second Continental Congress on July 4, 1776, establishing the United States of America. The Founding Father delegates of the Second Continental Congress declared that the Thirteen Colonies were no longer subject (and subordinate) to the monarch of Britain, King George III, and were now united, free, and independent states. The Congress voted to approve independence by passing the Lee Resolution on July 2 and adopted the Declaration of Independence two days later, on July 4.\"\n",
    "\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "what          2,054\n",
      "did           2,106\n",
      "the           1,996\n",
      "american      2,137\n",
      "people        2,111\n",
      "declare      13,520\n",
      "their         2,037\n",
      "independence  4,336\n",
      "from          2,013\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "independence  4,336\n",
      "day           2,154\n",
      ",             1,010\n",
      "known         2,124\n",
      "colloquially 23,992\n",
      "as            2,004\n",
      "the           1,996\n",
      "fourth        2,959\n",
      "of            1,997\n",
      "july          2,251\n",
      ",             1,010\n",
      "is            2,003\n",
      "a             1,037\n",
      "federal       2,976\n",
      "holiday       6,209\n",
      "in            1,999\n",
      "the           1,996\n",
      "united        2,142\n",
      "states        2,163\n",
      "which         2,029\n",
      "commemorates 25,530\n",
      "the           1,996\n",
      "ratification 27,369\n",
      "of            1,997\n",
      "the           1,996\n",
      "declaration   8,170\n",
      "of            1,997\n",
      "independence  4,336\n",
      "by            2,011\n",
      "the           1,996\n",
      "second        2,117\n",
      "continental   6,803\n",
      "congress      3,519\n",
      "on            2,006\n",
      "july          2,251\n",
      "4             1,018\n",
      ",             1,010\n",
      "1776         13,963\n",
      ",             1,010\n",
      "establishing  7,411\n",
      "the           1,996\n",
      "united        2,142\n",
      "states        2,163\n",
      "of            1,997\n",
      "america       2,637\n",
      ".             1,012\n",
      "the           1,996\n",
      "founding      4,889\n",
      "father        2,269\n",
      "delegates    10,284\n",
      "of            1,997\n",
      "the           1,996\n",
      "second        2,117\n",
      "continental   6,803\n",
      "congress      3,519\n",
      "declared      4,161\n",
      "that          2,008\n",
      "the           1,996\n",
      "thirteen      7,093\n",
      "colonies      8,355\n",
      "were          2,020\n",
      "no            2,053\n",
      "longer        2,936\n",
      "subject       3,395\n",
      "(             1,006\n",
      "and           1,998\n",
      "subordinate  15,144\n",
      ")             1,007\n",
      "to            2,000\n",
      "the           1,996\n",
      "monarch      11,590\n",
      "of            1,997\n",
      "britain       3,725\n",
      ",             1,010\n",
      "king          2,332\n",
      "george        2,577\n",
      "iii           3,523\n",
      ",             1,010\n",
      "and           1,998\n",
      "were          2,020\n",
      "now           2,085\n",
      "united        2,142\n",
      ",             1,010\n",
      "free          2,489\n",
      ",             1,010\n",
      "and           1,998\n",
      "independent   2,981\n",
      "states        2,163\n",
      ".             1,012\n",
      "the           1,996\n",
      "congress      3,519\n",
      "voted         5,444\n",
      "to            2,000\n",
      "approve      14,300\n",
      "independence  4,336\n",
      "by            2,011\n",
      "passing       4,458\n",
      "the           1,996\n",
      "lee           3,389\n",
      "resolution    5,813\n",
      "on            2,006\n",
      "july          2,251\n",
      "2             1,016\n",
      "and           1,998\n",
      "adopted       4,233\n",
      "the           1,996\n",
      "declaration   8,170\n",
      "of            1,997\n",
      "independence  4,336\n",
      "two           2,048\n",
      "days          2,420\n",
      "later         2,101\n",
      ",             1,010\n",
      "on            2,006\n",
      "july          2,251\n",
      "4             1,018\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]).to(device), # The tokens representing our input text.\n",
    "                             token_type_ids=torch.tensor([segment_ids]).to(device), # The segment IDs to differentiate question from answer_text\n",
    "                             return_dict=True) \n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did the American people declare their independence from?\n",
      "Answer: \"the monarch of britain\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying other models / Trying larger text sizes\n",
    "*full disclosure I am using the wiki from one of my favorite games project zomboid to source the large text input size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
    "\n",
    "# # Initialize Longformer tokenizer and model\n",
    "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# model = LongformerForQuestionAnswering.from_pretrained('allenai/longformer-base-4096')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/kaleb/.local/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = 'bert-large-uncased-whole-word-masking-finetuned-squad' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "# bert_model = './bert'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(bert_model)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own block because it's a huge chunk of text. Testing for scalibility (also it's my own built in lore searcher for one of my favorite games)\n",
    "question = \"What do the zombies follow?\"\n",
    "answer_text = \"A zombie is the player's main antagonist in Project Zomboid. These once human citizens of Knox Country roam the landscape in the thousands. They have an insatiable hunger for human flesh and will not hesitate to kill. The default zombies in Project Zomboid are inspired by George A. Romero's shambler zombies. The zombies can be heavily modified with the use of custom sandbox. Sandbox has many options for modifying the zombies, such as increasing or reducing their speed, primary senses, strength etc. Spawning: By default, zombies generally spawn in higher numbers at urban areas than rural areas. However, most areas won't have the exact same population. Some settlements will usually have larger zombie populations than others, such as Louisville. Downtown areas tend to have higher populations than suburban or outskirt areas. The population will increase and hit its peak after 30 days have passed. Some areas can also become more populated as time passes due to hordes migrating from other areas. Though sometimes the opposite may happen as zombies populating an area may wander away from the area too, such as from gunshots and other sounds from the metagame. Zombies can also spawn in enclosed spaces such as bathrooms or closets and ambush unsuspecting survivors. Behavior: Zombies feasting on an unfortunate victim. Zombies rely on their eyesight and hearing, they're especially drawn to noises such as radios, running vehicles and gunfire. If they hear a noise behind them, they will first look behind and turn around afterward. Zombies prioritize their pathfinding: the sight of prey attracts them first and foremost. With no human flesh in sight, noise is the next priority, regardless of whether it is man-made or not (e.g., thunder). Some zombies can be seen sitting against walls. This gives survivors an easier opportunity of killing that zombie or safely getting past, due to the zombie having to stand up first. When the player dies next to zombies, they will kneel down and begin to eat their corpse. Zombies can also occasionally be found feasting on a corpse, also giving survivors an easier chance of slipping past or killing the zombie(s). Hordes: Horde movement. Zombies tend to roam in hordes, with one of those zombies being the designated horde leader. The zombies in that horde will follow that leader around. Zombie hordes are the most dangerous, and are best avoided at any stage of the game.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 506 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "what          2,054\n",
      "do            2,079\n",
      "the           1,996\n",
      "zombies      14,106\n",
      "follow        3,582\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "a             1,037\n",
      "zombie       11,798\n",
      "is            2,003\n",
      "the           1,996\n",
      "player        2,447\n",
      "'             1,005\n",
      "s             1,055\n",
      "main          2,364\n",
      "antagonist   17,379\n",
      "in            1,999\n",
      "project       2,622\n",
      "z             1,062\n",
      "##om          5,358\n",
      "##bo          5,092\n",
      "##id          3,593\n",
      ".             1,012\n",
      "these         2,122\n",
      "once          2,320\n",
      "human         2,529\n",
      "citizens      4,480\n",
      "of            1,997\n",
      "knox         11,994\n",
      "country       2,406\n",
      "roam         25,728\n",
      "the           1,996\n",
      "landscape     5,957\n",
      "in            1,999\n",
      "the           1,996\n",
      "thousands     5,190\n",
      ".             1,012\n",
      "they          2,027\n",
      "have          2,031\n",
      "an            2,019\n",
      "ins          16,021\n",
      "##ati        10,450\n",
      "##able        3,085\n",
      "hunger        9,012\n",
      "for           2,005\n",
      "human         2,529\n",
      "flesh         5,771\n",
      "and           1,998\n",
      "will          2,097\n",
      "not           2,025\n",
      "hesitate     16,390\n",
      "to            2,000\n",
      "kill          3,102\n",
      ".             1,012\n",
      "the           1,996\n",
      "default      12,398\n",
      "zombies      14,106\n",
      "in            1,999\n",
      "project       2,622\n",
      "z             1,062\n",
      "##om          5,358\n",
      "##bo          5,092\n",
      "##id          3,593\n",
      "are           2,024\n",
      "inspired      4,427\n",
      "by            2,011\n",
      "george        2,577\n",
      "a             1,037\n",
      ".             1,012\n",
      "romero       18,290\n",
      "'             1,005\n",
      "s             1,055\n",
      "sham         25,850\n",
      "##bler       16,213\n",
      "zombies      14,106\n",
      ".             1,012\n",
      "the           1,996\n",
      "zombies      14,106\n",
      "can           2,064\n",
      "be            2,022\n",
      "heavily       4,600\n",
      "modified      6,310\n",
      "with          2,007\n",
      "the           1,996\n",
      "use           2,224\n",
      "of            1,997\n",
      "custom        7,661\n",
      "sand          5,472\n",
      "##box         8,758\n",
      ".             1,012\n",
      "sand          5,472\n",
      "##box         8,758\n",
      "has           2,038\n",
      "many          2,116\n",
      "options       7,047\n",
      "for           2,005\n",
      "modifying    29,226\n",
      "the           1,996\n",
      "zombies      14,106\n",
      ",             1,010\n",
      "such          2,107\n",
      "as            2,004\n",
      "increasing    4,852\n",
      "or            2,030\n",
      "reducing      8,161\n",
      "their         2,037\n",
      "speed         3,177\n",
      ",             1,010\n",
      "primary       3,078\n",
      "senses        9,456\n",
      ",             1,010\n",
      "strength      3,997\n",
      "etc           4,385\n",
      ".             1,012\n",
      "spawning     27,957\n",
      ":             1,024\n",
      "by            2,011\n",
      "default      12,398\n",
      ",             1,010\n",
      "zombies      14,106\n",
      "generally     3,227\n",
      "spawn        25,645\n",
      "in            1,999\n",
      "higher        3,020\n",
      "numbers       3,616\n",
      "at            2,012\n",
      "urban         3,923\n",
      "areas         2,752\n",
      "than          2,084\n",
      "rural         3,541\n",
      "areas         2,752\n",
      ".             1,012\n",
      "however       2,174\n",
      ",             1,010\n",
      "most          2,087\n",
      "areas         2,752\n",
      "won           2,180\n",
      "'             1,005\n",
      "t             1,056\n",
      "have          2,031\n",
      "the           1,996\n",
      "exact         6,635\n",
      "same          2,168\n",
      "population    2,313\n",
      ".             1,012\n",
      "some          2,070\n",
      "settlements   7,617\n",
      "will          2,097\n",
      "usually       2,788\n",
      "have          2,031\n",
      "larger        3,469\n",
      "zombie       11,798\n",
      "populations   7,080\n",
      "than          2,084\n",
      "others        2,500\n",
      ",             1,010\n",
      "such          2,107\n",
      "as            2,004\n",
      "louisville   11,577\n",
      ".             1,012\n",
      "downtown      5,116\n",
      "areas         2,752\n",
      "tend          7,166\n",
      "to            2,000\n",
      "have          2,031\n",
      "higher        3,020\n",
      "populations   7,080\n",
      "than          2,084\n",
      "suburban      9,282\n",
      "or            2,030\n",
      "outs         21,100\n",
      "##kir        23,630\n",
      "##t           2,102\n",
      "areas         2,752\n",
      ".             1,012\n",
      "the           1,996\n",
      "population    2,313\n",
      "will          2,097\n",
      "increase      3,623\n",
      "and           1,998\n",
      "hit           2,718\n",
      "its           2,049\n",
      "peak          4,672\n",
      "after         2,044\n",
      "30            2,382\n",
      "days          2,420\n",
      "have          2,031\n",
      "passed        2,979\n",
      ".             1,012\n",
      "some          2,070\n",
      "areas         2,752\n",
      "can           2,064\n",
      "also          2,036\n",
      "become        2,468\n",
      "more          2,062\n",
      "populated    10,357\n",
      "as            2,004\n",
      "time          2,051\n",
      "passes        5,235\n",
      "due           2,349\n",
      "to            2,000\n",
      "horde        21,038\n",
      "##s           2,015\n",
      "migrating    28,636\n",
      "from          2,013\n",
      "other         2,060\n",
      "areas         2,752\n",
      ".             1,012\n",
      "though        2,295\n",
      "sometimes     2,823\n",
      "the           1,996\n",
      "opposite      4,500\n",
      "may           2,089\n",
      "happen        4,148\n",
      "as            2,004\n",
      "zombies      14,106\n",
      "pop           3,769\n",
      "##ulating    10,924\n",
      "an            2,019\n",
      "area          2,181\n",
      "may           2,089\n",
      "wander       17,677\n",
      "away          2,185\n",
      "from          2,013\n",
      "the           1,996\n",
      "area          2,181\n",
      "too           2,205\n",
      ",             1,010\n",
      "such          2,107\n",
      "as            2,004\n",
      "from          2,013\n",
      "gunshot      22,077\n",
      "##s           2,015\n",
      "and           1,998\n",
      "other         2,060\n",
      "sounds        4,165\n",
      "from          2,013\n",
      "the           1,996\n",
      "meta         18,804\n",
      "##game       16,650\n",
      ".             1,012\n",
      "zombies      14,106\n",
      "can           2,064\n",
      "also          2,036\n",
      "spawn        25,645\n",
      "in            1,999\n",
      "enclosed     10,837\n",
      "spaces        7,258\n",
      "such          2,107\n",
      "as            2,004\n",
      "bathrooms    28,942\n",
      "or            2,030\n",
      "closet        9,346\n",
      "##s           2,015\n",
      "and           1,998\n",
      "ambush       15,283\n",
      "un            4,895\n",
      "##sus        13,203\n",
      "##pe          5,051\n",
      "##cting      11,873\n",
      "survivors     8,643\n",
      ".             1,012\n",
      "behavior      5,248\n",
      ":             1,024\n",
      "zombies      14,106\n",
      "feast         9,831\n",
      "##ing         2,075\n",
      "on            2,006\n",
      "an            2,019\n",
      "unfortunate  15,140\n",
      "victim        6,778\n",
      ".             1,012\n",
      "zombies      14,106\n",
      "rely         11,160\n",
      "on            2,006\n",
      "their         2,037\n",
      "eyes          2,159\n",
      "##ight       18,743\n",
      "and           1,998\n",
      "hearing       4,994\n",
      ",             1,010\n",
      "they          2,027\n",
      "'             1,005\n",
      "re            2,128\n",
      "especially    2,926\n",
      "drawn         4,567\n",
      "to            2,000\n",
      "noises       14,950\n",
      "such          2,107\n",
      "as            2,004\n",
      "radios       22,229\n",
      ",             1,010\n",
      "running       2,770\n",
      "vehicles      4,683\n",
      "and           1,998\n",
      "gunfire      16,978\n",
      ".             1,012\n",
      "if            2,065\n",
      "they          2,027\n",
      "hear          2,963\n",
      "a             1,037\n",
      "noise         5,005\n",
      "behind        2,369\n",
      "them          2,068\n",
      ",             1,010\n",
      "they          2,027\n",
      "will          2,097\n",
      "first         2,034\n",
      "look          2,298\n",
      "behind        2,369\n",
      "and           1,998\n",
      "turn          2,735\n",
      "around        2,105\n",
      "afterward     9,707\n",
      ".             1,012\n",
      "zombies      14,106\n",
      "prior         3,188\n",
      "##iti        25,090\n",
      "##ze          4,371\n",
      "their         2,037\n",
      "path          4,130\n",
      "##fin        16,294\n",
      "##ding        4,667\n",
      ":             1,024\n",
      "the           1,996\n",
      "sight         4,356\n",
      "of            1,997\n",
      "prey          8,336\n",
      "attracts     17,771\n",
      "them          2,068\n",
      "first         2,034\n",
      "and           1,998\n",
      "foremost     16,097\n",
      ".             1,012\n",
      "with          2,007\n",
      "no            2,053\n",
      "human         2,529\n",
      "flesh         5,771\n",
      "in            1,999\n",
      "sight         4,356\n",
      ",             1,010\n",
      "noise         5,005\n",
      "is            2,003\n",
      "the           1,996\n",
      "next          2,279\n",
      "priority      9,470\n",
      ",             1,010\n",
      "regardless    7,539\n",
      "of            1,997\n",
      "whether       3,251\n",
      "it            2,009\n",
      "is            2,003\n",
      "man           2,158\n",
      "-             1,011\n",
      "made          2,081\n",
      "or            2,030\n",
      "not           2,025\n",
      "(             1,006\n",
      "e             1,041\n",
      ".             1,012\n",
      "g             1,043\n",
      ".             1,012\n",
      ",             1,010\n",
      "thunder       8,505\n",
      ")             1,007\n",
      ".             1,012\n",
      "some          2,070\n",
      "zombies      14,106\n",
      "can           2,064\n",
      "be            2,022\n",
      "seen          2,464\n",
      "sitting       3,564\n",
      "against       2,114\n",
      "walls         3,681\n",
      ".             1,012\n",
      "this          2,023\n",
      "gives         3,957\n",
      "survivors     8,643\n",
      "an            2,019\n",
      "easier        6,082\n",
      "opportunity   4,495\n",
      "of            1,997\n",
      "killing       4,288\n",
      "that          2,008\n",
      "zombie       11,798\n",
      "or            2,030\n",
      "safely        9,689\n",
      "getting       2,893\n",
      "past          2,627\n",
      ",             1,010\n",
      "due           2,349\n",
      "to            2,000\n",
      "the           1,996\n",
      "zombie       11,798\n",
      "having        2,383\n",
      "to            2,000\n",
      "stand         3,233\n",
      "up            2,039\n",
      "first         2,034\n",
      ".             1,012\n",
      "when          2,043\n",
      "the           1,996\n",
      "player        2,447\n",
      "dies          8,289\n",
      "next          2,279\n",
      "to            2,000\n",
      "zombies      14,106\n",
      ",             1,010\n",
      "they          2,027\n",
      "will          2,097\n",
      "knee          6,181\n",
      "##l           2,140\n",
      "down          2,091\n",
      "and           1,998\n",
      "begin         4,088\n",
      "to            2,000\n",
      "eat           4,521\n",
      "their         2,037\n",
      "corpse       11,547\n",
      ".             1,012\n",
      "zombies      14,106\n",
      "can           2,064\n",
      "also          2,036\n",
      "occasionally  5,681\n",
      "be            2,022\n",
      "found         2,179\n",
      "feast         9,831\n",
      "##ing         2,075\n",
      "on            2,006\n",
      "a             1,037\n",
      "corpse       11,547\n",
      ",             1,010\n",
      "also          2,036\n",
      "giving        3,228\n",
      "survivors     8,643\n",
      "an            2,019\n",
      "easier        6,082\n",
      "chance        3,382\n",
      "of            1,997\n",
      "slipping     11,426\n",
      "past          2,627\n",
      "or            2,030\n",
      "killing       4,288\n",
      "the           1,996\n",
      "zombie       11,798\n",
      "(             1,006\n",
      "s             1,055\n",
      ")             1,007\n",
      ".             1,012\n",
      "horde        21,038\n",
      "##s           2,015\n",
      ":             1,024\n",
      "horde        21,038\n",
      "movement      2,929\n",
      ".             1,012\n",
      "zombies      14,106\n",
      "tend          7,166\n",
      "to            2,000\n",
      "roam         25,728\n",
      "in            1,999\n",
      "horde        21,038\n",
      "##s           2,015\n",
      ",             1,010\n",
      "with          2,007\n",
      "one           2,028\n",
      "of            1,997\n",
      "those         2,216\n",
      "zombies      14,106\n",
      "being         2,108\n",
      "the           1,996\n",
      "designated    4,351\n",
      "horde        21,038\n",
      "leader        3,003\n",
      ".             1,012\n",
      "the           1,996\n",
      "zombies      14,106\n",
      "in            1,999\n",
      "that          2,008\n",
      "horde        21,038\n",
      "will          2,097\n",
      "follow        3,582\n",
      "that          2,008\n",
      "leader        3,003\n",
      "around        2,105\n",
      ".             1,012\n",
      "zombie       11,798\n",
      "horde        21,038\n",
      "##s           2,015\n",
      "are           2,024\n",
      "the           1,996\n",
      "most          2,087\n",
      "dangerous     4,795\n",
      ",             1,010\n",
      "and           1,998\n",
      "are           2,024\n",
      "best          2,190\n",
      "avoided       9,511\n",
      "at            2,012\n",
      "any           2,151\n",
      "stage         2,754\n",
      "of            1,997\n",
      "the           1,996\n",
      "game          2,208\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]).to(device), # The tokens representing our input text.\n",
    "                             token_type_ids=torch.tensor([segment_ids]).to(device), # The segment IDs to differentiate question from answer_text\n",
    "                             return_dict=True) \n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do the zombies follow?\n",
      "Answer: \"the designated horde leader\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
