{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae67649-ba0d-4825-8eac-f6256f74ebc3",
   "metadata": {},
   "source": [
    "# Assignment: Notebooks\n",
    "### *Same text as lab.md/assignment.md* \n",
    "\n",
    "Using the notebooks associated to this lesson (either the PyTorch or the TensorFlow version), rerun them using your own dataset, perhaps one from Kaggle, used with attribution. Rewrite the notebook to underline your own findings. Try a different kind of dataset and document your findings, using text such as [this Kaggle competition dataset about weather tweets](https://www.kaggle.com/competitions/crowdflower-weather-twitter/data?select=train.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6221673-a1d8-4f67-a74c-f7434be08da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaleb/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import *\n",
    "from kalebtorchnlp import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import portalocker\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c70212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(df, target, test_size=0.2, random_state=42, printdf=False):\n",
    "#     if target not in df.columns:\n",
    "#         print(f\"Column '{target}' does not exist in the dataframe\")\n",
    "\n",
    "#     df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function']\n",
    "#     df['text'] = df['text'].str.lower()\n",
    "#     df.drop(['title', 'location', 'department', 'company_profile', 'description', 'requirements', 'benefits', 'employment_type', 'required_experience', 'required_education', 'industry', 'function'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#     if printdf == True:\n",
    "#         print(df.columns)\n",
    "#         print(df.head())\n",
    "#         print(df[target])\n",
    "    \n",
    "#     # train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "#     if target not in df.columns:\n",
    "#         print(f\"Column '{target}' does not exist in the dataframe\")\n",
    "#     train_df, test_df = train_test_split(df['text'], test_size = test_size , random_state = random_state)\n",
    "#     train_labels , test_labels = train_test_split(df[target], test_size = test_size , random_state = random_state)\n",
    "\n",
    "\n",
    "\n",
    "#     train_dataset = CSVDataset(train_df, target)\n",
    "#     test_dataset = CSVDataset(test_df, target)\n",
    "\n",
    "#     return train_dataset, test_dataset, train_labels, test_labels\n",
    "\n",
    "# datasetDir = 'data/datasets/fake_job_postings.csv'\n",
    "# target = \"fraudulent\"\n",
    "# df = pd.read_csv(datasetDir)\n",
    "\n",
    "# train_dataset, test_dataset = preprocess(df, target=target, printdf=True)\n",
    "\n",
    "# print(train_dataset[0])\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe33c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_column):\n",
    "        # print(dataframe.columns)\n",
    "        # print(dataframe[target_column])\n",
    "        self.dataframe = dataframe\n",
    "        self.target_column = target_column\n",
    "        self.features = dataframe.drop([target_column],axis=1 )\n",
    "        self.targets = dataframe[target_column]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features.iloc[idx].values.tolist()\n",
    "        target = self.targets.iloc[idx]\n",
    "        return features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5d5a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataframe after preprocessing: Index(['job_id', 'salary_range', 'telecommuting', 'has_company_logo',\n",
      "       'has_questions', 'fraudulent', 'text'],\n",
      "      dtype='object')\n",
      "First few rows of the dataframe:\n",
      "   job_id salary_range  telecommuting  has_company_logo  has_questions  \\\n",
      "0       1          NaN              0                 1              0   \n",
      "1       2          NaN              0                 1              0   \n",
      "2       3          NaN              0                 1              0   \n",
      "3       4          NaN              0                 1              0   \n",
      "4       5          NaN              0                 1              1   \n",
      "\n",
      "   fraudulent                                               text  \n",
      "0           0                                                NaN  \n",
      "1           0                                                NaN  \n",
      "2           0                                                NaN  \n",
      "3           0  account executive - washington dc us, dc, wash...  \n",
      "4           0                                                NaN  \n",
      "Target column data:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: fraudulent, dtype: int64\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CSVDataset' object has no attribute 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     36\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(datasetDir)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Print column names and first few rows for debugging\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(\"Columns in dataframe:\", df.columns)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"First few rows of the dataframe:\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(df.head())\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m train_dataset, test_dataset, train_labels, test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(df, target, test_size, random_state, printdf)\u001b[0m\n\u001b[1;32m     20\u001b[0m prep_df \u001b[38;5;241m=\u001b[39m CSVDataset(df, target)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_df, test_df, train_labels, test_labels \u001b[38;5;241m=\u001b[39m train_test_split(prep_df\u001b[38;5;241m.\u001b[39mfeatures, \u001b[43mprep_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m printdf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CSVDataset' object has no attribute 'target'"
     ]
    }
   ],
   "source": [
    "def preprocess(df, target, test_size=0.2, random_state=42, printdf=False):\n",
    "    if target not in df.columns:\n",
    "        raise KeyError(f\"Column '{target}' does not exist in the dataframe\")\n",
    "\n",
    "    # Combine text columns\n",
    "    df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function']\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    \n",
    "    # Drop original text columns\n",
    "    df.drop(['title', 'location', 'department', 'company_profile', 'description', 'requirements', 'benefits', 'employment_type', 'required_experience', 'required_education', 'industry', 'function'], axis=1, inplace=True)\n",
    "\n",
    "    if printdf == True:\n",
    "        print(\"Columns in dataframe after preprocessing:\", df.columns)\n",
    "        print(\"First few rows of the dataframe:\")\n",
    "        print(df.head())\n",
    "        print(\"Target column data:\")\n",
    "        print(df[target].head())\n",
    "    \n",
    "    # Assuming CSVDataset takes text and labels separately\n",
    "    prep_df = CSVDataset(df, target)\n",
    "\n",
    "    # Split the data\n",
    "    train_df, test_df, train_labels, test_labels = train_test_split(prep_df.features, prep_df.target, test_size=test_size, random_state=random_state)\n",
    "    if printdf == True:\n",
    "        print(f\"Train data shape: {train_df.shape}\")\n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        print(f\"Train labels shape: {train_labels.shape}\")\n",
    "        print(f\"Test labels shape: {test_labels.shape}\")    \n",
    "\n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "datasetDir = 'data/datasets/fake_job_postings.csv'\n",
    "target = 'fraudulent'\n",
    "df = pd.read_csv(datasetDir)\n",
    "\n",
    "# Print column names and first few rows for debugging\n",
    "# print(\"Columns in dataframe:\", df.columns)\n",
    "# print(\"First few rows of the dataframe:\")\n",
    "# print(df.head())\n",
    "\n",
    "train_dataset, test_dataset, train_labels, test_labels = preprocess(df, target=target, printdf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18669f-cb5b-4ebd-83e1-37f5e7df619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'data/datasets'\n",
    "datasetDir = dataDir + '/fake_job_postings.csv'\n",
    "\n",
    "classes = ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'telecommuting']\n",
    "\n",
    "train_dataset, test_dataset, vocab = load_dataset(datasetDir, target, classes)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# df = pd.read_csv(datasetDir)\n",
    "\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# vocab = torchtext.vocab.vocab(collections.Counter(), min_freq=1)\n",
    "# vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40aaaa-bf5e-4d25-9407-0e9f171ec725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24c7fc-fb6d-47a0-9800-1636d4ffe08e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     32\u001b[0m target_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraudulent\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 34\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Test length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess(df, target, test_size=0.2, random_state=42, printdf=False):\n",
    "    if target not in df.columns:\n",
    "        print(f\"Column '{target}' does not exist in the dataframe\")\n",
    "\n",
    "    df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function']\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df.drop(['title', 'location', 'department', 'company_profile', 'description', 'requirements', 'benefits', 'employment_type', 'required_experience', 'required_education', 'industry', 'function'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    if printdf == True:\n",
    "        print(df.columns)\n",
    "        print(df.head())\n",
    "        print(df[target])\n",
    "    \n",
    "    # train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    if target not in df.columns:\n",
    "        print(f\"Column '{target}' does not exist in the dataframe\")\n",
    "\n",
    "    prep_df = CSVDataset(df, target)\n",
    "    \n",
    "    train_df, test_df = train_test_split(prep_df, test_size = test_size , random_state = random_state)\n",
    "    # train_labels , test_labels = train_test_split(df[target], test_size = test_size , random_state = random_state)\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = CSVDataset(train_df, target)\n",
    "    test_dataset = CSVDataset(test_df, target)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "batch_size = 32\n",
    "target_column = 'fraudulent'\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f\"Train length: {len(train_dataloader)}. Test length: {len(test_dataloader)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cefdc8-a52f-4480-b60f-bc51c87af618",
   "metadata": {},
   "source": [
    "# Simple RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1943c-ef75-4a7d-9bcf-74c0583af6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc11cef-3f07-4148-aa5d-03427f5b7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1da88-35e6-4596-abf6-654169a3f946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
