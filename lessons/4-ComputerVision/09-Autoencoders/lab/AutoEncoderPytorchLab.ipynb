{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b6d76c-ba80-47bd-adb5-b5c0715a1c66",
   "metadata": {},
   "source": [
    "# Lab document created and seperated from the original code to make things run better. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f03e787-fea0-4641-89ba-4f26a465f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c587b2-d544-4e01-8a2e-d4858d8cbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "train_size = 0.9\n",
    "lr = 1e-3\n",
    "eps = 1e-8\n",
    "batch_size = 256\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128d0566-07dc-4963-878e-8aa4a46a2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloaders, model, loss_fn, optimizer, epochs, device, noisy=None, super_res=None):\n",
    "    tqdm_iter = tqdm(range(epochs))\n",
    "    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n",
    "\n",
    "    for epoch in tqdm_iter:\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            imgs, labels = batch\n",
    "            shapes = list(imgs.shape)\n",
    "\n",
    "            if super_res is not None:\n",
    "                shapes[2], shapes[3] = int(shapes[2] / super_res), int(shapes[3] / super_res)\n",
    "                _transform = transforms.Resize((shapes[2], shapes[3]))\n",
    "                imgs_transformed = _transform(imgs)\n",
    "                imgs_transformed = imgs_transformed.to(device)\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if noisy is not None:\n",
    "                noisy_tensor = noisy[0]\n",
    "            else:\n",
    "                noisy_tensor = torch.zeros(tuple(shapes)).to(device)\n",
    "\n",
    "            if super_res is None:\n",
    "                imgs_noisy = imgs + noisy_tensor\n",
    "            else:\n",
    "                imgs_noisy = imgs_transformed + noisy_tensor\n",
    "\n",
    "            imgs_noisy = torch.clamp(imgs_noisy, 0., 1.)\n",
    "\n",
    "            preds = model(imgs_noisy)\n",
    "            loss = loss_fn(preds, imgs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                imgs, labels = batch\n",
    "                shapes = list(imgs.shape)\n",
    "\n",
    "                if super_res is not None:\n",
    "                    shapes[2], shapes[3] = int(shapes[2] / super_res), int(shapes[3] / super_res)\n",
    "                    _transform = transforms.Resize((shapes[2], shapes[3]))\n",
    "                    imgs_transformed = _transform(imgs)\n",
    "                    imgs_transformed = imgs_transformed.to(device)\n",
    "\n",
    "\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if noisy is not None:\n",
    "                    test_noisy_tensor = noisy[1]\n",
    "                else:\n",
    "                    test_noisy_tensor = torch.zeros(tuple(shapes)).to(device)\n",
    "\n",
    "                if super_res is None:\n",
    "                    imgs_noisy = imgs + test_noisy_tensor\n",
    "                else:\n",
    "                    imgs_noisy = imgs_transformed + test_noisy_tensor\n",
    "\n",
    "                imgs_noisy = torch.clamp(imgs_noisy, 0., 1.)\n",
    "\n",
    "                preds = model(imgs_noisy)\n",
    "                loss = loss_fn(preds, imgs)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n",
    "        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n",
    "        tqdm_iter.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25317c46-2f69-489e-8b13-9e856854e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(dataloaders, model, optimizer, epochs, device):\n",
    "    tqdm_iter = tqdm(range(epochs))\n",
    "    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n",
    "\n",
    "    for epoch in tqdm_iter:\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            imgs, labels = batch\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            z_vals = model.get_zvals()\n",
    "            loss = vae_loss(preds, imgs, z_vals)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                imgs, labels = batch\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(imgs)\n",
    "                z_vals = model.get_zvals()\n",
    "                loss = vae_loss(preds, imgs, z_vals)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n",
    "        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n",
    "        tqdm_iter.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b143d3d-11f3-4bf5-b96b-53545170642d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b44904-eb27-4239-82aa-9c3c57577f7f",
   "metadata": {},
   "source": [
    "> **Task 1**: Try to train autoencoder with very small latent vector size, eg. 2, and plot the dots corresponding to different digits. *Hint: Use fully-connected dense layer after the convoluitonal part to reduce the vector size to the required value.*\n",
    "\n",
    "> **Task 2**: Starting from different digits, obtain their latent space representations, and see what effect adding some noise to the latent space has on the resulting digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a0e782-e915-4836-aa64-8753df727607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "KeiAutoEncoder                           [1, 1, 28, 28]            --\n",
      "├─KeiEncoder: 1-1                        [1, 2]                    --\n",
      "│    └─Conv2d: 2-1                       [1, 16, 28, 28]           160\n",
      "│    └─ReLU: 2-2                         [1, 16, 28, 28]           --\n",
      "│    └─MaxPool2d: 2-3                    [1, 16, 14, 14]           --\n",
      "│    └─Conv2d: 2-4                       [1, 8, 14, 14]            1,160\n",
      "│    └─ReLU: 2-5                         [1, 8, 14, 14]            --\n",
      "│    └─MaxPool2d: 2-6                    [1, 8, 7, 7]              --\n",
      "│    └─Conv2d: 2-7                       [1, 8, 7, 7]              584\n",
      "│    └─ReLU: 2-8                         [1, 8, 7, 7]              --\n",
      "│    └─MaxPool2d: 2-9                    [1, 8, 4, 4]              --\n",
      "│    └─Linear: 2-10                      [1, 2]                    258\n",
      "├─KeiDecoder: 1-2                        [1, 1, 28, 28]            --\n",
      "│    └─Linear: 2-11                      [1, 128]                  384\n",
      "│    └─ReLU: 2-12                        [1, 128]                  --\n",
      "│    └─Conv2d: 2-13                      [1, 8, 4, 4]              584\n",
      "│    └─ReLU: 2-14                        [1, 8, 4, 4]              --\n",
      "│    └─Upsample: 2-15                    [1, 8, 8, 8]              --\n",
      "│    └─Conv2d: 2-16                      [1, 8, 8, 8]              584\n",
      "│    └─ReLU: 2-17                        [1, 8, 8, 8]              --\n",
      "│    └─Upsample: 2-18                    [1, 8, 16, 16]            --\n",
      "│    └─Conv2d: 2-19                      [1, 16, 14, 14]           1,168\n",
      "│    └─ReLU: 2-20                        [1, 16, 14, 14]           --\n",
      "│    └─Upsample: 2-21                    [1, 16, 28, 28]           --\n",
      "│    └─Conv2d: 2-22                      [1, 1, 28, 28]            145\n",
      "│    └─Sigmoid: 2-23                     [1, 1, 28, 28]            --\n",
      "==========================================================================================\n",
      "Total params: 5,027\n",
      "Trainable params: 5,027\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.77\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.18\n",
      "==========================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, eps\u001b[38;5;241m=\u001b[39meps)\n\u001b[1;32m     59\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m---> 60\u001b[0m train(\u001b[43mdataloaders\u001b[49m, model, loss_fn, optimizer, epochs, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "class KeiEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding='same')\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=(3, 3), padding='same')\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), padding=(1, 1))\n",
    "        self.fc1 = nn.Linear(8*4*4, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = self.maxpool1(self.relu(self.conv1(input)))\n",
    "        hidden2 = self.maxpool2(self.relu(self.conv2(hidden1)))\n",
    "        encoded = self.maxpool3(self.relu(self.conv3(hidden2)))\n",
    "        flat = encoded.view(encoded.size(0), -1)\n",
    "        linear = self.fc1(flat)\n",
    "        return linear\n",
    "        \n",
    "class KeiDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 8 * 4 * 4)\n",
    "        self.conv1 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n",
    "        self.upsample1 = nn.Upsample(scale_factor=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n",
    "        self.upsample2 = nn.Upsample(scale_factor=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=(3, 3))\n",
    "        self.upsample3 = nn.Upsample(scale_factor=(2, 2))\n",
    "        self.conv4 = nn.Conv2d(16, 1, kernel_size=(3, 3), padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        linear = self.relu(self.fc1(input))\n",
    "        linear = linear.view(linear.size(0), 8, 4, 4)\n",
    "        hidden1 = self.upsample1(self.relu(self.conv1(linear)))\n",
    "        hidden2 = self.upsample2(self.relu(self.conv2(hidden1)))\n",
    "        hidden3 = self.upsample3(self.relu(self.conv3(hidden2)))\n",
    "        decoded = self.sigmoid(self.conv4(hidden3))\n",
    "        return decoded\n",
    "        \n",
    "class KeiAutoEncoder(nn.Module):\n",
    "    def __init__(self, super_resolution=False):\n",
    "        super().__init__()\n",
    "        self.encoder = KeiEncoder()\n",
    "        self.decoder = KeiDecoder()\n",
    "\n",
    "    def forward(self, input):\n",
    "        encoded = self.encoder(input)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "model = KeiAutoEncoder()\n",
    "print(summary(model,input_size=(1,1,28,28)))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n",
    "loss_fn = nn.BCELoss()\n",
    "train(dataloaders, model, loss_fn, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1596f2-2fa6-468f-8a97-eea9068d8966",
   "metadata": {},
   "source": [
    "> **Task**: In our sample, we have trained fully-connected VAE. Now take the CNN from traditional auto-encoder above and create CNN-based VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c7e23-2621-4432-bb08-f1f985526818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
