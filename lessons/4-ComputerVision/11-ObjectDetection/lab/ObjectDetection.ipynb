{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851d7613-b989-4cba-82ed-43fd445d4889",
   "metadata": {},
   "source": [
    "# Head Detection using Hollywood Heads Dataset\n",
    "\n",
    "Lab Assignment from [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).\n",
    "\n",
    "## Task\n",
    "\n",
    "Counting number of people on video surveillance camera stream is an important task that will allow us to estimate the number of visitors in a shops, busy hours in a restaurant, etc. To solve this task, we need to be able to detect human heads from different angles. To train object detection model to detect human heads, we can use [Hollywood Heads Dataset](https://www.di.ens.fr/willow/research/headdetection/).\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "[Hollywood Heads Dataset](https://www.di.ens.fr/willow/research/headdetection/release/HollywoodHeads.zip) contains 369,846 human heads annotated in 224,740 movie frames from Hollywood movies. It is provided in [https://host.robots.ox.ac.uk/pascal/VOC/](PASCAL VOC) format, where for each image there is also an XML description file that looks like this:\n",
    "\n",
    "```xml\n",
    "<annotation>\n",
    "\t<folder>HollywoodHeads</folder>\n",
    "\t<filename>mov_021_149390.jpeg</filename>\n",
    "\t<source>\n",
    "\t\t<database>HollywoodHeads 2015 Database</database>\n",
    "\t\t<annotation>HollywoodHeads 2015</annotation>\n",
    "\t\t<image>WILLOW</image>\n",
    "\t</source>\n",
    "\t<size>\n",
    "\t\t<width>608</width>\n",
    "\t\t<height>320</height>\n",
    "\t\t<depth>3</depth>\n",
    "\t</size>\n",
    "\t<segmented>0</segmented>\n",
    "\t<object>\n",
    "\t\t<name>head</name>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>201</xmin>\n",
    "\t\t\t<ymin>1</ymin>\n",
    "\t\t\t<xmax>480</xmax>\n",
    "\t\t\t<ymax>263</ymax>\n",
    "\t\t</bndbox>\n",
    "\t\t<difficult>0</difficult>\n",
    "\t</object>\n",
    "\t<object>\n",
    "\t\t<name>head</name>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>3</xmin>\n",
    "\t\t\t<ymin>4</ymin>\n",
    "\t\t\t<xmax>241</xmax>\n",
    "\t\t\t<ymax>285</ymax>\n",
    "\t\t</bndbox>\n",
    "\t\t<difficult>0</difficult>\n",
    "\t</object>\n",
    "</annotation>\n",
    "```\n",
    "\n",
    "In this dataset, there is only one class of objects `head`, and for each head, you get the coordinates of the bounding box. You can parse XML using Python libraries, or use [this library](https://pypi.org/project/pascal-voc/) to deal directly with PASCAL VOC format.\n",
    "\n",
    "## Training Object Detection \n",
    "\n",
    "You can train an object detection model using one of the following ways:\n",
    "\n",
    "* Using [Azure Custom Vision](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/quickstarts/object-detection?tabs=visual-studio&WT.mc_id=academic-77998-cacaste) and it's Python API to programmatically train the model in the cloud. Custom vision will not be able to use more than a few hundred images for training the model, so you may need to limit the dataset.\n",
    "* Using the example from [Keras tutorial](https://keras.io/examples/vision/retinanet/) to train RetunaNet model.\n",
    "* Using [torchvision.models.detection.RetinaNet](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/retinanet.html) build-in module in torchvision.\n",
    "\n",
    "## Takeaway\n",
    "\n",
    "Object detection is a task that is frequently required in industry. While there are some services that can be used to perform object detection (such as [Azure Custom Vision](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/quickstarts/object-detection?tabs=visual-studio&WT.mc_id=academic-77998-cacaste)), it is important to understand how object detection works and to be able to train your own models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa93e3-8b3d-48ba-bdea-73f4ca6afae5",
   "metadata": {},
   "source": [
    "# Kaleb's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af05c16-aca2-4bf6-9cad-646bb622b269",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pascal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# from pascal import annotation_from_xml\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpascal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_xml\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pascal'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "from pascal import annotation_from_xml\n",
    "from pascal.utils import save_xml\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4228a83-8701-4db3-80b3-cee5f668cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "train_size = 0.9\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-6\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a0286-1e35-4aa0-987d-2b941f412c07",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460355f-f1d4-4519-ab1a-5b39909230e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'HollywoodHeads'\n",
    "img_path = os.path.join(dataset_path,'JPEGImages')\n",
    "ann_path = os.path.join(dataset_path,'Annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075d290-5480-453d-ae75-7a47595320cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
