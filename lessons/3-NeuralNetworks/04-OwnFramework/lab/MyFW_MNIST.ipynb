{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification with our own Framework\n",
    "\n",
    "Lab Assignment from [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).\n",
    "\n",
    "### Reading the Dataset\n",
    "\n",
    "This code download the dataset from the repository on the internet. You can also manually copy the dataset from `/data` directory of AI Curriculum repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm *.pkl\n",
    "# !wget https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/data/mnist.pkl.gz\n",
    "# !gzip -d mnist.pkl.gz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "import pylab\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('mnist.pkl','rb') as f:\n",
    "#     MNIST = pickle.load(f)\n",
    "\n",
    "\n",
    "with gzip.open('../../../data/mnist.pkl.gz', 'rb') as mnist_pickle:\n",
    "    MNIST = pickle.load(mnist_pickle, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MNIST[0][1]\n",
    "data = MNIST[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the shape of data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "We will use Scikit Learn to split the data between training and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 40000, test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "print(f\"Train samples: {len(features_train)}, test samples: {len(features_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaleb Code\n",
    "Let's start with the framework of the implementation (provided by the lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework\n",
    "class Linear:\n",
    "    def __init__(self,nin,nout):\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))\n",
    "        self.b = np.zeros((1,nout))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x=x\n",
    "        return np.dot(x, self.W.T) + self.b\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dx = np.dot(dz, self.W)\n",
    "        dW = np.dot(dz.T, self.x)\n",
    "        db = dz.sum(axis=0)\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "    \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db\n",
    "    \n",
    "class Softmax:\n",
    "    def forward(self,z):\n",
    "        self.z = z\n",
    "        zmax = z.max(axis=1,keepdims=True)\n",
    "        expz = np.exp(z-zmax)\n",
    "        Z = expz.sum(axis=1,keepdims=True)\n",
    "        return expz / Z\n",
    "    def backward(self,dp):\n",
    "        p = self.forward(self.z)\n",
    "        pdp = p * dp\n",
    "        return pdp - p * pdp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "class CrossEntropyLoss:\n",
    "    def forward(self,p,y):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        p_of_y = p[np.arange(len(y)), y]\n",
    "        log_prob = np.log(p_of_y)\n",
    "        return -log_prob.mean()\n",
    "    def backward(self,loss):\n",
    "        dlog_softmax = np.zeros_like(self.p)\n",
    "        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)\n",
    "        return dlog_softmax / self.p\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self,x):\n",
    "        y = np.tanh(x)\n",
    "        self.y = y\n",
    "        return y\n",
    "    def backward(self,dy):\n",
    "        return (1.0-self.y**2)*dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framework is established where x (input) -> linear layer -> softmax layer -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the net to allow for easier layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self,l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self,z):\n",
    "        for l in self.layers[::-1]:\n",
    "            z = l.backward(z)\n",
    "        return z\n",
    "    \n",
    "    def update(self,lr):\n",
    "        for l in self.layers:\n",
    "            if 'update' in l.__dir__():\n",
    "                l.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=0, pos correct=0.0, neg correct=1.0\n",
      "Iteration=10, pos correct=0.7530413625304136, neg correct=0.9193219135528534\n",
      "Iteration=20, pos correct=0.9939172749391727, neg correct=0.4278645602201118\n",
      "Iteration=30, pos correct=0.6672749391727494, neg correct=0.965873790716251\n",
      "Iteration=40, pos correct=0.9912814274128142, neg correct=0.7125676755125588\n",
      "Iteration=50, pos correct=0.9860097323600974, neg correct=0.7829502085737108\n",
      "Iteration=60, pos correct=0.8169099756690997, neg correct=0.9752596077039141\n",
      "Iteration=70, pos correct=0.9845904298459043, neg correct=0.8475193041625987\n",
      "Iteration=80, pos correct=0.9738442822384428, neg correct=0.904300168634064\n",
      "Iteration=90, pos correct=0.9738442822384428, neg correct=0.904300168634064\n",
      "Iteration=0, pos correct=0.0, neg correct=1.0\n",
      "Iteration=10, pos correct=0.5901725959845016, neg correct=0.9953747574567935\n",
      "Iteration=20, pos correct=0.9711165903487143, neg correct=0.9244618925138758\n",
      "Iteration=30, pos correct=0.9711165903487143, neg correct=0.9244618925138758\n",
      "Iteration=40, pos correct=0.9711165903487143, neg correct=0.9244618925138758\n",
      "Iteration=50, pos correct=0.690207819654808, neg correct=0.9914715039935021\n",
      "Iteration=60, pos correct=0.9614300810144417, neg correct=0.965344524164072\n",
      "Iteration=70, pos correct=0.9614300810144417, neg correct=0.965344524164072\n",
      "Iteration=80, pos correct=0.9614300810144417, neg correct=0.965344524164072\n",
      "Iteration=90, pos correct=0.9614300810144417, neg correct=0.965344524164072\n",
      "Iteration=0, pos correct=0.0, neg correct=1.0\n",
      "Iteration=10, pos correct=0.9126409017713365, neg correct=0.7590602238408243\n",
      "Iteration=20, pos correct=0.7347020933977456, neg correct=0.9174364896073903\n",
      "Iteration=30, pos correct=0.7347020933977456, neg correct=0.9174364896073903\n",
      "Iteration=40, pos correct=0.9301529790660226, neg correct=0.7365206963936756\n",
      "Iteration=50, pos correct=0.8723832528180354, neg correct=0.863052940131462\n",
      "Iteration=60, pos correct=0.8607085346215781, neg correct=0.9022472908154201\n",
      "Iteration=70, pos correct=0.75, neg correct=0.9719088648072481\n",
      "Iteration=80, pos correct=0.6324476650563607, neg correct=0.9828122224196127\n",
      "Iteration=90, pos correct=0.9106280193236715, neg correct=0.824613608100906\n"
     ]
    }
   ],
   "source": [
    "def train(positive_examples, negative_examples, num_iterations = 100):\n",
    "    num_dims = positive_examples.shape[1]\n",
    "    weights = np.zeros((num_dims,1)) # initialize weights\n",
    "    \n",
    "    pos_count = positive_examples.shape[0]\n",
    "    neg_count = negative_examples.shape[0]\n",
    "    \n",
    "    report_frequency = 10\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        pos = random.choice(positive_examples)\n",
    "        neg = random.choice(negative_examples)\n",
    "\n",
    "        z = np.dot(pos, weights)   \n",
    "        if z < 0:\n",
    "            weights = weights + pos.reshape(weights.shape)\n",
    "\n",
    "        z  = np.dot(neg, weights)\n",
    "        if z >= 0:\n",
    "            weights = weights - neg.reshape(weights.shape)\n",
    "            \n",
    "        if i % report_frequency == 0:             \n",
    "            pos_out = np.dot(positive_examples, weights)\n",
    "            neg_out = np.dot(negative_examples, weights)        \n",
    "            pos_correct = (pos_out >= 0).sum() / float(pos_count)\n",
    "            neg_correct = (neg_out < 0).sum() / float(neg_count)\n",
    "            print(\"Iteration={}, pos correct={}, neg correct={}\".format(i,pos_correct,neg_correct))\n",
    "\n",
    "    return weights\n",
    "\n",
    "def set_mnist_pos_neg(positive_label, negative_label):\n",
    "    # KALEB CODE\n",
    "    # Train = 0; Features = 0 || Labels = 1\n",
    "    positive_indices = [i for i, j in enumerate(MNIST[0][1])\n",
    "                          if j == positive_label]\n",
    "    negative_indices = [i for i, j in enumerate(MNIST[0][1])\n",
    "                          if j == negative_label]\n",
    "\n",
    "\n",
    "    positive_images = MNIST[0][0][positive_indices]\n",
    "    negative_images = MNIST[0][0][negative_indices]\n",
    "    \n",
    "    return positive_images, negative_images\n",
    "\n",
    "\n",
    "# All vs one function that compares the one_label (one) against all other_labels (all)\n",
    "def set_mnist_all_vs_one(one_label, count = 10):\n",
    "    # Initialize the positive and negative images arrays\n",
    "    pos_imgs = []\n",
    "    neg_imgs = []\n",
    "    # For loop that goes through and finds the positive and negative images for our label (one_label) and every other label in the data set\n",
    "    for x in range(count):\n",
    "        if x != one_label:\n",
    "            pos, neg = set_mnist_pos_neg(one_label, x)\n",
    "            # Once we found them, add to full list of positive images and negative images\n",
    "            pos_imgs.extend(pos)\n",
    "            neg_imgs.extend(neg)\n",
    "            \n",
    "    return np.array(pos_imgs), np.array(neg_imgs)    \n",
    "\n",
    "\n",
    "perceptrons = []\n",
    "for i in range(3):\n",
    "    # print(\"*************************************************************************************\")\n",
    "    # print(f\"Gathering images for perceptron {i}\")\n",
    "    positive_images, negative_images = set_mnist_all_vs_one(i)\n",
    "    # print(f\"Training perceptron {i}\")\n",
    "    perceptrons.append(train(positive_images, negative_images))\n",
    "\n",
    "def classify(digit, perceptron_weights):  \n",
    "    predictedDigit = -1 # initialize to a digit that doesn't exist to keep track of not found\n",
    "    maxZ = float('-inf') # initialize to a 0 confidence as anything else that is predicted as the correct digit will be higher than 0\n",
    "    for i in range(len(perceptron_weights)):\n",
    "        z = np.dot(digit, perceptron_weights[i]) # Find the confidence of our guess\n",
    "        if (z >= maxZ): # If it's the best we've seen so far, let's go with that one\n",
    "            maxZ = z\n",
    "            predictedDigit = i\n",
    "\n",
    "    return predictedDigit, maxZ\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self,nin,nout):\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))\n",
    "        self.b = np.zeros((1,nout))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x=x\n",
    "        return np.dot(x, self.W.T) + self.b\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dx = np.dot(dz, self.W)\n",
    "        dW = np.dot(dz.T, self.x)\n",
    "        db = dz.sum(axis=0)\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "    \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "### Starting with just 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(Linear(784,10))\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):\n",
    "    for i in range(0,len(train_x),batch_size):\n",
    "        xb = train_x[i:i+batch_size]\n",
    "        yb = train_labels[i:i+batch_size]\n",
    "\n",
    "        p = net.forward(xb)\n",
    "        l = loss.forward(p,yb)\n",
    "        dp = loss.backward(l)\n",
    "        dx = net.backward(dp)\n",
    "        net.update(lr)\n",
    "\n",
    "train_epoch(net,features_train,labels_train, loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss=0.3484966662336011, accuracy=0.899725: \n",
      "Test loss=0.3834930576854378, accuracy=0.8942: \n"
     ]
    }
   ],
   "source": [
    "def get_loss_acc(x,y,loss=CrossEntropyLoss()):\n",
    "    p = net.forward(x)\n",
    "    l = loss.forward(p,y)\n",
    "    pred = np.argmax(p,axis=1)\n",
    "    acc = (pred==y).mean()\n",
    "    return l,acc\n",
    "print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(features_train,labels_train)))\n",
    "print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(features_test,labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(Linear(784,100))\n",
    "net.add(Tanh())\n",
    "net.add(Linear(100,10))\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss=0.18393319869763414, accuracy=0.9414: \n",
      "Test loss=0.2228993248726967, accuracy=0.9308: \n"
     ]
    }
   ],
   "source": [
    "train_epoch(net,features_train,labels_train, loss)\n",
    "print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(features_train,labels_train)))\n",
    "print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(features_test,labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(Linear(784,100))\n",
    "net.add(Tanh())\n",
    "net.add(Linear(100,100))\n",
    "net.add(Tanh())\n",
    "net.add(Linear(100,10))\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss=0.22471452550598606, accuracy=0.933: \n",
      "Test loss=0.2721654854951008, accuracy=0.9266: \n"
     ]
    }
   ],
   "source": [
    "train_epoch(net,features_train,labels_train, loss)\n",
    "print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(features_train,labels_train)))\n",
    "print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(features_test,labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
